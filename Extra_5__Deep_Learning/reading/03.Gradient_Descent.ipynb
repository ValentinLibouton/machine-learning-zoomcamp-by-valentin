{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent \n",
    "\n",
    "Tout le monde a déjà entendu parler de l'algorithme de descente de gradient, mais... savez-vous vraiment comment il fonctionne et l'avez-vous déjà mis en œuvre par vous-même pour vous assurer que vous avez bien compris son fonctionnement ?\n",
    "\n",
    "Utiliser des modules qui font les calculs à notre place, c'est bien.\n",
    "\n",
    "Comprendre ce que l'on manipule, c'est mieux !\n",
    "\n",
    "C'est ce que nous allons faire dans cet article, en trois étapes :\n",
    "\n",
    "1. Qu'est-ce que la descente de gradient ?\n",
    "2. Comment fonctionne-t-elle ?\n",
    "3. Et quels sont les pièges à éviter ?\n",
    "\n",
    "Les seuls prérequis pour cet article sont de savoir ce qu'est une dérivée.\n",
    "\n",
    "Allons-y !\n",
    "\n",
    "## What is gradient descent? \n",
    "\n",
    "Il s'agit d'un algorithme permettant de trouver le minimum d'une fonction.\n",
    "\n",
    "C'est un problème que l'on retrouve partout en mathématiques.\n",
    "\n",
    "Et c'est aussi le cas en science des données, notamment lorsqu'on veut minimiser le taux d'erreur qui est redressé lors de la rétropropagation.\n",
    "\n",
    "**La descente de gradient propose une approche:**\n",
    "\n",
    "- algorithmique\n",
    "- itérative\n",
    "- qui fonctionne assez bien dans la plupart des cas\n",
    "\n",
    "Parfois, nous pouvons avoir [vanish gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), mais nous y reviendrons plus tard.\n",
    "\n",
    "### How does it work?  \n",
    "Imaginez que vous êtes au sommet d'une montagne, sans guide, sans carte, et que vous voulez descendre au point le plus bas de la montagne (c'est-à-dire là où l'altitude est la plus basse). (c'est-à-dire là où l'altitude est la plus basse).\n",
    "\n",
    "Votre approche se fera face à la pente, et vous irez dans cette direction pendant quelques minutes.\n",
    "\n",
    "![mountain](./img/montagne.png)\n",
    "\n",
    "A few minutes later, you are at a new point.\n",
    "\n",
    "Again, you face the slope and move in that direction for a few minutes.\n",
    "\n",
    "And so on...\n",
    "\n",
    "And after a while, always going down, you will go to the lowest point of altitude.\n",
    "\n",
    "Easy!\n",
    "\n",
    "### How does it work mathematically?\n",
    "\n",
    "In math, the slope is the derivative:\n",
    "![](./img/derivate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The value of the derivative corresponds to the inclination of the slope at a given point.\n",
    "\n",
    "Donc...\n",
    "\n",
    "- Si la dérivée est élevée, cela signifie que la pente est très forte.\n",
    "- Et si la dérivée est faible, la pente est faible.\n",
    "- Si la dérivée est égale à 0, la pente est plate. \n",
    "- Et si la dérivée est négative, cela signifie qu'elle descend (quand on va vers la droite !).\n",
    "\n",
    "Alors, une fois qu'on a la valeur de la pente, comment fait-on pour descendre ?\n",
    "\n",
    "On fait le tour de la pente dans l'autre sens !\n",
    "\n",
    "Dérivée positive => pente qui monte vers la droite => on va vers la gauche.\n",
    "Dérivée négative => pente descendante vers la droite => on va vers la droite.\n",
    "\n",
    "Mais de combien ?\n",
    "\n",
    "Faut-il faire un pas, deux pas, continuer pendant combien de temps ?\n",
    "\n",
    "En fait, on aimerait faire un seul pas, reformuler la question de la dérivée, puis faire un autre pas, etc.\n",
    "\n",
    "Sauf que cela va être très intensif en termes de calcul (nous allons prendre beaucoup de décisions) si nous faisons cela.\n",
    "\n",
    "Mais d'un autre côté, si on fait de grands pas, on risque de rater le minimum, donc de revenir dans l'autre sens, de dépasser à nouveau le minimum, et ainsi de suite, sans jamais tomber dessus.\n",
    "\n",
    "Il suffit de trouver le bon équilibre !\n",
    "\n",
    "Pour ce faire, il faut spécifier ce que l'on appelle un taux d'apprentissage. Nous en reparlerons un peu plus tard.\n",
    "\n",
    "### More concretely ...\n",
    "\n",
    "Essayons de trouver le point le plus bas de cette courbe. L'objectif est de trouver le minimum que l'on voit à droite, autour de x entre 3 et 4.\n",
    "\n",
    "![](./img/function.png)\n",
    "\n",
    "Dans ce cas, nous pourrions calculer la dérivée égale à 0, mais l'objectif est de comprendre la descente du gradient, c'est donc ce que nous allons faire.\n",
    "\n",
    "Consider the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-19T10:28:49.106830443Z",
     "start_time": "2024-01-19T10:28:48.882012541Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def f(x):\n",
    "    return 2 * x * x * np.cos(x) - 5 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et étudions-le sur l'intervalle [-5,5] :\n",
    "\n",
    "Il y a 3 étapes :\n",
    "\n",
    "1. On prend un point au hasard x0.\n",
    "2. On calcule la valeur de la pente à f(x0).\n",
    "3. On se déplace dans la direction opposée à la pente. \n",
    "\n",
    "#### Première étape :\n",
    "On prend un point au hasard, ici x0=-1. Cela correspond à f(x0)=6,08."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-19T10:28:49.108459117Z",
     "start_time": "2024-01-19T10:28:49.105845685Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "6.0806046117362795"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [-1.]\n",
    "f(x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the curve, it represents the following \n",
    "![](./img/function-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2:\n",
    "Calculate the value of the slope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-19T10:28:49.109099446Z",
     "start_time": "2024-01-19T10:28:49.106483529Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "-5.478267253856766"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def df(x):\n",
    "    return 4 * x * np.cos(x) - 2 * x * x * np.sin(x) - 5\n",
    "\n",
    "slope = df(x[0])\n",
    "slope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons donc une pente négative égale à ``-5.47``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 \n",
    " We move in the opposite direction to the slope:  \n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
    "  <msub>\n",
    "    <mi>x</mi>\n",
    "    <mn>1</mn>\n",
    "  </msub>\n",
    "  <mo>=</mo>\n",
    "  <msub>\n",
    "    <mi>x</mi>\n",
    "    <mn>0</mn>\n",
    "  </msub>\n",
    "  <mo>&#x2212;<!-- − --></mo>\n",
    "  <mi>&#x03B1;<!-- α --></mi>\n",
    "  <mo>&#x2217;<!-- ∗ --></mo>\n",
    "  <msup>\n",
    "    <mi>f</mi>\n",
    "    <mo>&#x2032;</mo>\n",
    "  </msup>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <msub>\n",
    "    <mi>x</mi>\n",
    "    <mn>0</mn>\n",
    "  </msub>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "</math>\n",
    "\n",
    "How to choose the value of ``α`` ?\n",
    "\n",
    "This is the learning rate. I propose for the moment to test a small value α=0.05, and we will test other values a little later. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-19T10:28:49.109684122Z",
     "start_time": "2024-01-19T10:28:49.106835673Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "-0.7260866373071617"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.05\n",
    "x.append(x[0] - alpha * slope)\n",
    "x[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our new value for our point! Let's display it:\n",
    "![](./img/function-3.png)\n",
    "\n",
    "\n",
    "We moved a little bit. This approach is iterative, which means that the operation will have to be repeated several times to achieve the minimum.\n",
    "\n",
    "Let's start over:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-19T10:28:49.110855697Z",
     "start_time": "2024-01-19T10:28:49.107181115Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "-0.4024997370140509"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.append(x[1] - alpha * df(x[1]))\n",
    "x[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/function-4.png)\n",
    "\n",
    "We're moving slowly.\n",
    "\n",
    "After a little over a dozen iterations, our algorithm converges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-19T10:28:49.111813145Z",
     "start_time": "2024-01-19T10:28:49.107526326Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[-1.0,\n -0.7260866373071617,\n -0.4024997370140509,\n -0.08477906213634434,\n 0.18205499002642517,\n 0.39684580640116923,\n 0.5797318757542436,\n 0.7511409760238664,\n 0.929843593497496,\n 1.1379425635322518,\n 1.4100262396071885,\n 1.8111367982460322,\n 2.4659523010837896,\n 3.481091120446543,\n 3.9840239754024296,\n 3.5799142362878964,\n 3.9342838641256046,\n 3.6341484369757358,\n 3.900044342976242,\n 3.670089111844099,\n 3.8747793435314155]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [-1.]\n",
    "for i in range(20):\n",
    "    x.append(x[i] - alpha * df(x[i]))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/gradientdescent-alpha0.05.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our little ball ends up reaching the minimum and staying there, at about x=3.8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les pièges à éviter\n",
    "\n",
    "Voilà, nous avons vu le principe de la descente de gradient.\n",
    "\n",
    "Sur un exemple simple.\n",
    "\n",
    "Mais dans la réalité, il arrive souvent que l'on rencontre des problèmes. Par exemple :\n",
    "\n",
    "- Comment fixer le taux d'apprentissage ?\n",
    "- Comment résoudre le problème du gradient qui s'évanouit ?\n",
    "- Comment lutter contre les minima locaux ?\n",
    "\n",
    "\n",
    "Comment fixer le taux d'apprentissage ?\n",
    "Dans l'exemple précédent, nous nous sommes fixé α=0,05.\n",
    "\n",
    "Pourquoi ? \n",
    "\n",
    "J'ai un peu triché.\n",
    "\n",
    "Je l'ai testé avant et j'ai vu que c'était une valeur qui fonctionnait bien.\n",
    "\n",
    "En fait, il faut trouver le bon équilibre en tenant compte de cela :\n",
    "\n",
    "Plus la valeur α est élevée, plus on avancera vite, mais l'algorithme risque de ne jamais converger.\n",
    "Plus la valeur α est petite, plus on avancera lentement, et donc plus il faudra de temps pour converger.  \n",
    "\n",
    "Par exemple, avec une valeur α=0,2, on obtient :\n",
    "\n",
    "![](./img/gradientdescent-alpha0.2.gif)\n",
    "\n",
    "Nous voyons maintenant que nous avons un problème de convergence. La valeur α est trop élevée.\n",
    "\n",
    "Par conséquent, lorsque le skieur fait face à la pente, il avance tellement qu'il se retrouve de l'autre côté de la montagne.\n",
    "\n",
    "Si l'on prend une valeur trop petite, comme α=0.001\n",
    "\n",
    "![](./img/gradientdescent-alpha0.001.gif)\n",
    "\n",
    "Cela fonctionne, cela converge, mais cela prend beaucoup plus de temps ! (L'animation est accélérée 5 fois plus vite)\n",
    "\n",
    "Pour notre problème simple, l'impact est faible, mais lorsque vous entraînez des réseaux neuronaux, cela fait une grande différence en termes de temps de calcul !\n",
    "\n",
    "Malheureusement, il n'existe pas de formule magique pour trouver le taux d'apprentissage parfait.\n",
    "\n",
    "Pour le trouver, il faut en tester plusieurs et obtenir le meilleur.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  How to solve the problem of the vanishing gradient?\n",
    "\n",
    "Il existe deux problèmes très courants dans le domaine de l'apprentissage profond : l'explosion du gradient et la disparition du gradient. Dans le premier cas, il s'agit d'un taux d'apprentissage trop élevé qui entraîne l'instabilité de l'algorithme. Dans le cas de l'apprentissage profond, cela peut se produire lorsque le réseau est très étendu. Comme les gradients de chaque couche sont multipliés entre eux, on peut très vite avoir un gradient qui explose de manière exponentielle. Dans le cas d'un gradient qui disparaît, c'est l'inverse.\n",
    "\n",
    "Le gradient devient tellement faible que notre skieur ne progresse pratiquement pas. Cela peut arriver si le taux d'apprentissage est trop faible, comme nous l'avons vu. Mais cela peut aussi se produire si notre skieur est bloqué sur une sorte de plateau.\n",
    "\n",
    "Imaginez la fonction suivante :\n",
    "Comment résoudre le problème de l'évanouissement du gradient ?\n",
    "\n",
    "Il existe deux problèmes très courants en Deep Learning, l'explosion du gradient et la disparition du gradient.\n",
    "\n",
    "Dans le premier cas, il s'agit d'un taux d'apprentissage trop élevé qui entraîne l'instabilité de l'algorithme.\n",
    "\n",
    "Dans le cas du Deep Learning, cela peut se produire lorsque le réseau est très étendu. Comme les gradients de chaque couche sont multipliés entre eux, on peut très vite avoir un gradient qui explose de manière exponentielle.\n",
    "\n",
    "Dans le cas d'un gradient qui disparaît, c'est l'inverse.\n",
    "\n",
    "Le gradient devient tellement faible que notre skieur ne progresse pratiquement pas. Cela peut arriver si le taux d'apprentissage est trop faible, comme nous l'avons vu.\n",
    "\n",
    "Mais cela peut aussi se produire si notre skieur est bloqué sur une sorte de plateau.\n",
    "\n",
    "Imaginons la fonction suivante :\n",
    "\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n",
    "  <mi>f</mi>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <mi>x</mi>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "  <mo>=</mo>\n",
    "  <mi>arctan</mi>\n",
    "  <mo>&#x2061;</mo>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <msup>\n",
    "    <mi>x</mi>\n",
    "    <mn>2</mn>\n",
    "  </msup>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "</math>\n",
    "\n",
    "![plateau](./img/plate.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the minimum is in x=0.\n",
    "\n",
    "Suppose that the random point falls on a value far from 0, such as -20.\n",
    "\n",
    "I choose a very high learning rate compared to the previous examples, at α=1, and I get a very long algorithm to converge:\n",
    "\n",
    "![vanish](./img/vanish.gif) \n",
    "\n",
    "But imagine a more complex function that is a mixture of this one, with a long flat plateau, and roller coasters in other places. At that point, whatever the learning rate you choose, you will have problems. Either a very slow convergence or an instability of the algorithm.\n",
    "\n",
    "In the Deep Learning, this type of problem is solved with the ReLU functions. We'll see about that later.\n",
    "\n",
    "### How to fight against local minima?\n",
    "Let us consider this time the following function:\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n",
    "  <mi>f</mi>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <mi>x</mi>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "  <mo>=</mo>\n",
    "  <mi>x</mi>\n",
    "  <mi>cos</mi>\n",
    "  <mo>&#x2061;<!-- ⁡ --></mo>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <mi>x</mi>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "</math>\n",
    "\n",
    "![](./img/loc.png)\n",
    "\n",
    "The problem with this function is that there are many local minima, i. e. troughs, which are not the global minimum (which, over this interval, is on the right, around x=9.5).\n",
    "\n",
    "The following animation shows some examples where the starting point varies:\n",
    "![](./img/var.gif)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the convergence point depends a lot on the initial point.\n",
    "\n",
    "Sometimes he will be able to find the global minimum, and other times, the algorithm will get stuck in a local minimum.\n",
    "\n",
    "One technique to avoid this problem is to run the algorithm several times,  \n",
    "and to keep the smallest of the minima, but obviously it is more intensive for the cpu"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
